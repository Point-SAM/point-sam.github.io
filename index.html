<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Point-SAM: Promptable 3D Segmentation Model for Point Clouds</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.4/css/bulma.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.24/dist/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.24/dist/js/bulma-carousel.min.js"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/python.min.js"></script>
    <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/latex.min.js"></script> -->

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha512-MV7K8+y+gLIBoVD59lQIYicR65iaqukzvf/nwasF0nqhPay5w/9lJmVM2hMDcnK1OnMGCdVK+iQrJ7lzPJQd1w==" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>

<body>
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1"><b>Point-SAM</b>: Promptable 3D Segmentation Model for Point Clouds</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://github.com/zyc00">Yuchen Zhou</a><sup>* 1</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://cseweb.ucsd.edu/~jigu/">Jiayuan Gu</a><sup>* 1</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://github.com/tungyen">Tung Yen Chiang</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.fbxiang.com">Fanbo Xiang</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://cseweb.ucsd.edu/~haosu/">Hao Su</a><sup>1,2</sup>,
                            </span>
                        </div>
                        <div class=""is-size-6 publication-authors">
                            <span class="author-block">
                                UC San Diego<sup>1</sup>,
                            </span>
                            <span class="author-block">
                                Hillbot<sup>2</sup>
                            </span>
                        </div>
                    </div>
                </div>
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <span class="link-block">
                                <a class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="ai ai-arxiv"></i>
                                    </span>
                                    <span>arXiv</span>
                                </a>
                            </span>
                            <span class="link-block">
                                <a href="https://github.com/zyc00/Point-SAM" class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="fa fa-github"></i>
                                    </span>
                                    <span>GitHub</span>
                                </a>
                            </span>
                            <span class="link-block">
                                <a href="https://huggingface.co/spaces/yuchen0187/Point-SAM" class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="fa-solid fa-cog fa-spin fa-spin-reverse"></i>
                                    </span>
                                    <span>Demo</span>
                                </a>
                            </span>
                            <span class="link-block">
                                <a href="https://github.com/zyc00/point-sam-demo/tree/main/examples" class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="fa fa-download"></i>
                                    </span>
                                    <span>Examples</span>
                                </a>
                            </span>
                        </div>
                    </div>
                </div>
                <div class="content">
                    <b>Introduction: </b>Due to advancements in robotics and graphics, there is a growing demand for 3D data with detailed part-level annotations. However, manual annotation of 3D point clouds and meshes is exceedingly laborious and time-consuming. To address this challenge, we introduce <b>Point-SAM</b>, a transformer-based 3D segmentation model designed to incorporate interactive guidance through point prompts. <b>Point-SAM</b> takes both a point cloud and user-provided prompts as inputs, generating precise segmentation masks as outputs. The following videos demonstrate how <b>Point-SAM</b> operates on out-of-distribution meshes.
                </div>
                
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <div class="content" style="font-style: italic;">
                            Try our demo here
                            <a href="https://huggingface.co/spaces/yuchen0187/Point-SAM"><span style="display: inline-block; margin: 0.25em; border: 1px solid rgb(95, 10, 185); border-radius: 0.25em; padding: .25em 0.5em"><i class="fa-solid fa-arrow-right fa-fw" title="Forward"></i></span></a>
                        </div>
                    </div>
                </div>

                <div class="carousel results-carousel" data-autoplay-speed="10000">
                    <img src="./static/img/indoor-ezgif.com-video-to-gif-converter.gif" align="left" width="48%" />
                    <img src="./static/img/outdoor-ezgif.com-video-to-gif-converter.gif" align="right" width="48%" />
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-widescreen">
            <div class="columns is-centered">
                <div class="column is-four-fifths has-text-centered">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            The development of 2D foundation models for image segmentation has been significantly advanced by the Segment Anything Model (SAM). However, achieving similar success in 3D models remains a challenge due to issues such as non-unified data formats, lightweight models, and the scarcity of labeled data with diverse masks. To this end, we propose a 3D promptable segmentation model <b>Point-SAM</b> focusing on point clouds. Our approach utilizes a transformer-based method, extending SAM to the 3D domain. We leverage part-level and object-level annotations and introduce a data engine to generate pseudo labels from SAM, thereby distilling 2D knowledge into our 3D model. Our model outperforms state-of-the-art models on several indoor and outdoor benchmarks and demonstrates a variety of applications, such as 3D annotation.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-widescreen">
            <div class="columns is-centered">
                <div class="column is-four-fifths has-text-centered">
                    <h2 class="title is-3">Overview</h2>
                    <img src="./static/img/PointSAM-Teaser.001.jpeg" width="100%" />
                    <div class="content has-text-justified">
                        <p>
                            <b>Point-SAM Model</b> The point cloud and prompts are feed into the model, and the model generates the segmentation mask. 
                            The point cloud is first grouped by FPS and KNN, then tokenized by a small PointNet. Then the tokens are feed into a ViT model.
                            The prompts are encoded by sin-cos positional encoding and then fused with the ViT's output. Finally, the fused output is decoded into the segmentation mask.
                        </p>
                        <p>
                            <b>Pseudo Label Engine</b> Initially, a relatively weak Point-SAM is trained on a mixture of existing datasets. Using both pre-trained Point-SAM and SAM, we generate pseudo labels by rendering RGB-D images of each mesh from six fixed camera positions and fusing these into a colored point cloud. For each 2D proposal, a corresponding 3D proposal is identified and refined by lifting a randomly sampled 2D prompt to a 3D prompt, prompting Point-SAM to predict a 3D mask on the fused point cloud, with the process repeated and modified at other views.
                            Details and results are provided in the figure below.
                        </p>
                        <p>
                            <b>Applications</b> Our model has a variety of applications, such as 3D annotation, where users can provide prompts to guide the model to generate precise segmentation masks. It can be used for diverse data sources, such as indoor and outdoor scenes as well as synthetic data. Our model can also be used for zero-shot instance proposal.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-widescreen">
            <div class="columns is-centered">
                <div class="column is-four-fifths has-text-centered">
                    <h2 class="title is-3">Pseudo Label Generation</h2>
                    <img src="./static/img/PointSAM-Teaser.003.jpeg" width="100%" />
                    <div class="content has-text-justified">
                        <p>
                            <b>Pseudo label generation and refinement process</b> A weak Point-SAM is trained on a mixture of existing datasets. We then use both pre-trained Point-SAM and SAM to generate pseudo labels. For each mesh, we render RGB-D images from six fixed camera positions and fuse them into a colored point cloud. SAM generates diverse 2D proposals for each view, and for each 2D proposal, we aim to find a corresponding 3D proposal. Starting from the view of the 2D proposal, a randomly sampled 2D prompt is lifted to a 3D prompt, prompting Point-SAM to predict a 3D mask on the fused point cloud. The next 2D prompt is sampled from the error region between the 2D proposal and the projection of the 3D proposal at this view. New 3D prompts and previous 3D proposal masks update the 3D proposal. This process repeats until the IoU between the 2D and 3D proposals exceeds a threshold, ensuring 3D-consistent segmentation regularized by Point-SAM while retaining SAM's diversity. We repeat this process with modifications at other views to refine the 3D proposal further. If the IoU is below a threshold, the 3D proposal is discarded, and the previous 3D proposal mask is used to prompt Point-SAM at each iteration, refining the 3D masks by incorporating 2D priors from SAM through space carving.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-widescreen">
            <div class="columns is-centered">
                <div class="column is-four-fifths has-text-centered">
                    <h2 class="title is-3">Prompt Segmentation Demos</h2>
                    <div class="content has-text-justified">
                        <p>
                            We show videos for prompt segmentation. Our demo can be found <a href="https://huggingface.co/spaces/yuchen0187/Point-SAM">here</a>.
                            Those objects are out-of-distribution meshes, the first three are downloaded from Objaverse, and the last one is from Polycam which is also 
                            an out-door scene with complex geomotry. You can find the example geomotries in the annotation deme <a href="https://github.com/zyc00/point-sam-demo">GitHub repo</a>.
                        </p>
                    </div>
                    <p>
                        <b>Video 1: Rhino from Objaverse</b>
                    </p>
                    <video width="100%" autoplay muted style="margin-bottom: 5%;">
                        <source src="./static/img/video_1.mp4#t=3,500" type="video/mp4" />
                    </video>

                    <p>
                        <b>Video 2: Transformer from Objaverse</b>
                    </p>
                    <video width="100%" autoplay muted style="margin-bottom: 5%;">
                        <source src="./static/img/video_2.mp4#t=3,500" type="video/mp4" />
                    </video>

                    <p>
                        <b>Video 3: Headphone from Objaverse</b>
                    </p>
                    <video width="100%" autoplay muted style="margin-bottom: 5%;">
                        <source src="./static/img/video_3.mp4#t=3,500" type="video/mp4" />
                    </video>

                    <p>
                        <b>Video 4: Castle from Polycam</b>
                    </p>
                    <video width="100%" autoplay muted style="margin-bottom: 5%;">
                        <source src="./static/img/video_4.mp4#t=13,500" type="video/mp4" />
                    </video>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-widescreen">
            <div class="columns is-centered">
                <div class="column is-four-fifths has-text-centered">
                    <h2 class="title is-3">Quantitative Results</h2>
                    <div class="content has-text-justified">
                        <p>
                            <b>Baseline</b> We compare Point-SAM with a multi-view extension of SAM, named MV-SAM, and a 3D interactive segmentation method, AGILE3D. Inspired by previous works, we introduce MV-SAM for zero-shot point-prompted segmentation as a strong baseline by rendering multi-view RGB-D images from each mesh. SAM is prompted at each view with simulated clicks from the error region center between SAM’s prediction and the 2D ground truth mask, with predictions lifted back to a sparse point cloud and merged into a single mask. For both MV-SAM and our method, the most confident prediction is selected if there are multiple outputs. AGILE3D uses a sparse convolutional U-Net, is trained on real-world scans of ScanNet40, and requires input scale adjustment for CAD models to ensure consistent performance.
                        </p>
                        <p>
                            <b>Results</b> Point-SAM shows superior zero-shot transferability and effectively handles data with different point counts and sources. It significantly outperforms MV-SAM, especially with few point prompts, demonstrating greater prompt efficiency. SAM struggles with multi-view consistency without extra fine-tuning, particularly with limited prompts. Point-SAM also surpasses AGILE3D across all datasets, notably in out-of-distribution scenarios like PartNet-Mobility and KITTI360, highlighting its strong zero-shot transferability. Figure 4 illustrates the qualitative comparison, showing Point-SAM's superior quality with a single prompt and significantly faster convergence compared to AGILE3D and MV-SAM.
                        </p>
                    </div>
                    <img src="./static/img/poster.002.jpeg" width="100%" />
                </div>
            </div>
        </div>
    </section>
</body>
</html>